#Delete existing cluster if it exists
kind delete cluster --name tamlab

>>
>>Create kubernetes v1.20 cluster with kind:

>> first, make sure you are logged into docker hub to pull the new kubernetes image
>> Use your dockerhub username and password to authenticate
% docker login
Authenticating with existing credentials...

>> Now, let's create a KIND cluster using a specific version of Kubernetes (v1.20.2) in this case

% kind create cluster --name tamlab --config kind-config.yaml  --image kindest/node:v1.20.2

>>  you should get something similar to the following output:
Creating cluster "tamlab" ...
 ✓ Ensuring node image (kindest/node:v1.20.2) 🖼 
⢄⡱ Preparing nodes 📦 📦 📦  
 ✓ Preparing nodes 📦 📦 📦  
 ✓ Writing configuration 📜 
 ✓ Starting control-plane 🕹️ 
 ✓ Installing CNI 🔌 
 ✓ Installing StorageClass 💾 
 ✓ Joining worker nodes 🚜 
Set kubectl context to "kind-tamlab"
You can now use your cluster with:

kubectl cluster-info --context kind-tamlab

>> Note:  if you get the following error, keep trying!  Docker is throttling requests
Error response from daemon: received unexpected HTTP status: 503 Service Unavailable



#View available clusters
kind get clusters

#Switch contexts commands
kubectl config get-contexts
kubectl config current-context
kubectl config set-context kind-tamlab

#or set explicitly in kubectl command
kubectl --context kind-tamlab

>>
>> container to container messaging
>> add example here along with slide

>> 
>> maybe use rabbitmq to show message queue across pods??

>>
>> add better slide to show cluster IP
>>

#demo ClusterIP service
kubectl apply -f 01hello-deploy.yaml
kubectl apply -f 01hello-svc.yaml
kubectl get all
>> see on which node the pods are running
>> also, look at labels, ports, secrets, volumes, where image is pulled from
kubectl describe pods

#note service type and internal IP
#what subnet is the service on vs the pod?
#find pod IP
kubectl describe pod backend-<suffix>

#show output from Pod
kubectl port-forward backend-<suffix> 8080:80
#open new command window
atauber-a02:~ atauber$ curl localhost:8080
{"message":"Hello"}

#show output from a ubuntu pod accessing the service
#Deploy a ubuntu pod for network and curl tests
kubectl run my-shell -i --tty --image ubuntu -- /bin/bash

#install packages into ubuntu
apt update, apt install curl

#access the service from ubuntu pod
#the hello service is a load balancer in front of the 3 replica pods
#we are in the same namespace so don't need fqdn
curl hello:80

>>
>> show better slide describing NodePort

#switch service to NodePort
kubectl get svc
- note: Type=ClusterIP
kubectl delete svc hello
kubectl apply -f 01hello-svc2.yaml
kubectl get all
- note:  Type=NodePort, also note that the Port is 30001

#note service type, IP, and Port mapping

#show output from NodePort
#note that in most environments, the command would be: "curl <K8s node IP>:30001"
#kind is forwarding the port from the docker K8s container to localhost
curl localhost:30001

>> show accessing service from external ip

>> show using load balancer (nginx or avi)?

>> show using coreDNS to access the service


#istio demo

#if using a downloaded istio distribution other than what is in this repo, you will
#need to adjust the /manifests/profiles/demo.yaml file to add the specific NodePorts: 31080 and 31443
- port: 80
  name: http2
  nodePort: 31080
  targetPort: 8080
- port: 443
  name: https
  nodePort: 31443
  targetPort: 8443

#export istioctl path
cd /istio-1.8.2
export PATH=$PWD/bin:$PATH
which istioctl

#install istio into cluster - this uses modified demo profile
istioctl install -f manifests/profiles/demo.yaml

#add namespace label so that new pods will get the istio sidecar
kubectl label namespace default istio-injection=enabled

#deploy the sample application
kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml
kubectl get all

#install gateway
kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml

#verify access
open browser to http://localhost:31080/productpage

#install kiali
kubectl apply -f samples/addons
kubectl rollout status deployment/kiali -n istio-system

#access kiali
istioctl dashboard kiali

#demo service mesh
#note bookinfo example: https://istio.io/latest/docs/examples/bookinfo/
refresh the productpage multiple times - note how the ratings change as the
3 different ratings versions get called

#route all traffic to v1 version
#create VirtualService to route traffic:https://istio.io/latest/docs/tasks/traffic-management/request-routing/
kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml
refresh productpage and note only v1 rating comes back

#cleanup
kubectl delete -f samples/bookinfo/networking/virtual-service-all-v1.yaml

#demo traffic shifting - 50/50
#route all to v1
kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml
refresh productpage to verify all going to v1

#apply 50/50 virtual Service
kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml
refresh productpage to verify a 50% mix to v1 and v3

#read through other traffic management options: https://istio.io/latest/docs/tasks/traffic-management/


>>
>> Add CNI antrea
>>

